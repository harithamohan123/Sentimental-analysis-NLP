# -*- coding: utf-8 -*-
"""Sentimental analysis using NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jnmCAYPhstZjLABVeIVqlhr9YZxY7jDs

Install natural language toolkit
"""

!pip install nltk

"""Import Libraries"""

import numpy as np
import pandas as pd
import re #Regular expressiom
import nltk
import matplotlib.pyplot as plt

from nltk.corpus import stopwords

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

from sklearn.model_selection import train_test_split

"""Load dataset from local directory"""

from google.colab import files
uploaded = files.upload()

"""Load and summarize dataset"""

dataset = pd.read_csv('dataset.csv')
print(dataset.shape)
print(dataset.head(5))

"""Segregating dataset as input and output"""

feature = dataset.iloc[: , 10].values
labels = dataset.iloc[: , 1].values
print(labels)

"""Removing special character"""

processed_features  = []

for sentence in range(0, len(feature)):
  # Remove all the special characters
  processed_feature = re.sub(r'\W',' ',str(feature[sentence]))

  # Remove all single characters
  processed_feature = re.sub(r'\s+[a-zA-Z]\s+', ' ', processed_feature)

  # Remove single characters from the start
  processed_feature = re.sub(r'\^[a-zA-Z]\s+', ' ', processed_feature) 

  # Substituting multiple spaces with single space
  processed_feature = re.sub(r'\s+', ' ', processed_feature, flags=re.I)

  # Removing prefixed 'b'
  processed_feature = re.sub(r'^b\s+', '', processed_feature)

  # Converting to Lowercase
  processed_feature = processed_feature.lower()

  processed_features.append(processed_feature)

"""Feature extraxtion form text"""

nltk.download('stopwords')
Vectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))
processed_features = Vectorizer.fit_transform(processed_features).toarray()
print(processed_features)

"""Splitting dataset into train and test"""

X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)

"""Implemennting Random forest algorithm"""

text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)
text_classifier.fit(X_train, y_train)

"""Predicting test data with trained model"""

predictions = text_classifier.predict(X_test)

"""Score of the model"""

print(accuracy_score(y_test,predictions))

"""Confusion matrix"""

from sklearn import metrics
import itertools
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes)
    plt.yticks(tick_marks, classes)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

cm = metrics.confusion_matrix(y_test, predictions, labels=['negative', 'neutral', 'positive'])
plot_confusion_matrix(cm, classes=['negative', 'neutral', 'positive'])

